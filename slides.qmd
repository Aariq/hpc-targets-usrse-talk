---
title: "Harnessing the Power of HPC From the Comfort of {{< iconify devicon:rstudio >}}"
author: "Eric R. Scott\n\n Univeristy of Arizona"
affiliation: "University of Arizona"
date: 2024-10-17
format: 
  uaz-revealjs:
    mermaid: 
      theme: forest
logo: logo.png
code-annotations: hover
link-external-newwindow: true
chalkboard: true
---

## My Background

<!--
12 min talk with 3 min for questions 
Suggestions from reviewers of abstract include:
- More details about overhead created by targets / efficiency
- Who is using `targets`? (3.8k scripts on GitHub contain "library(targets)", 768 files called "_targets.R")
- What similar projects are there? 
-->

::: incremental
-   Ecologist ‚Üí "Scientific Programmer & Educator"

-   {{< iconify devicon:rstudio >}} = my comfort zone ‚ù§Ô∏è

-   Attempted (unsuccessfully) to use HPC as PhD student

-   Successfully used HPC as postdoc
:::

::: notes
As a PhD student I was more motivated than most to get out of my comfort zone when it came to writing code, but even so, I tried to never leave the comfort of the RStudio IDE unless absolutely necessary.
:::

## Barriers to HPC use

-   Requirement to use shell commands

-   Using R without an IDE

-   Not seeing HPC resources as "for me"

::: notes
Even though I could use shell commands and type R code into the terminal, I didn't feel comfortable and it was like taking a *huge* leap backwards.

I had code that *could* run on my laptop (if I leave it on for a week with the fan at top speed), so is it even worth it to figure out HPC?\

\
I think there are *many* researchers like me, especially in life sciences.
:::

## Technologies to bridge the gap

Key skills that empowered me to use HPC while minimizing time outside of my comfort zone:

::: {.fragment .semi-fade-out}
1.  GitHub

2.  `renv` üì¶ for managing R package dependencies
:::

3.  Open OnDemand

4.  `targets` üì¶

::: notes
1.  GitHub‚Äîgenerally useful to academics, but also a good way to get code onto HPC

2.  `renv` and `pak` R packages‚Äîbetter package management for R

3.  Open OnDemand‚ÄîRStudio server running on HPC back-end

4.  `targets`‚Äîworkflow management with extensions to use HPC for parallelization

I'm going to focus on Open OnDemand and `targets`
:::

## ![](images/ood-logo.svg){width="250"}

::::: columns
::: column
-   RStudio IDE[^1] in a web browser
-   Code run on HPC cores
:::

::: column
[![](images/ood-screenshot.png){fig-align="right"}](Launch%20a%20fully%20functional%20RStudio%20IDE%20in%20a%20web%20browser%20backed%20by%20HPC%20cores)
:::
:::::

## ![](images/ood-logo.svg){width="250"}

I can avoid the command line *entirely*:

::: incremental
-   RStudio file pane for upload/download of files

-   RStudio git pane for interacting with git/GitHub

-   Run parallel R code on HPC cores without SLURM

-   Cons: can't load additional modules (?)
:::

## `renv` {.smaller visibility="hidden"}

`renv` makes R projects isolated, portable, and reproducible

1.  Run `renv::init()` locally (or use `renv::snapshot()` if already using `renv`)
2.  Get project on HPC (e.g. via GitHub)
3.  `renv` bootstraps itself and prompts user to run `renv::restore()`

![](images/renv-screenshot.png){width="769"}

## `pak` {visibility="hidden"}

::::: columns
::: {.column width="50%"}
-   `pak` offers alternative to `install.packages()`

-   Faster and aware of system dependencies

-   `renv` can optionally use `pak` for package installation
:::

::: {.column width="50%"}
![](images/pak-screenshot.png){width="639"}
:::
:::::

::: notes
Why is this such a great combination?
Limits the number of commands and amount of time I have to spend in the shell.
`ssh` into the HPC, `git clone` my project, `module R` and then `R`.
`renv` bootstraps itself and `renv::restore()` installs all my R packages.
:::

## `targets`

-   Make-like workflow management package for R
-   Skips computationally-intensive steps that are already up to date
-   Orchestrates parallel computing

## `targets`

``` {.r filename="_targets.R"}
library(targets)
tar_source()                                               # <1>
tar_option_set(packages = c("readr", "dplyr", "ggplot2"))  # <2>
list(                                                      # <3>
  tar_target(file, "data.csv", format = "file"),           # <3>
  tar_target(data, get_data(file)),                        # <3>
  tar_target(model, fit_model(data)),                      # <3>
  tar_target(plot, plot_model(model, data))                # <3>
)                                                          # <3>
```

1.  Sources all R scripts in `R/`
2.  Define packages needed for pipeline and other options
3.  Define pipeline

## `targets`

Visualize pipeline with `tar_visnetwork()`

<style>
.vis-output {
  background-color: white;
  border-radius: 4px;
}
</style>

```{r}
#| classes: vis-output
targets::tar_dir({
  targets::tar_script({
    library(targets)
    get_data <- function(x) x
    fit_model <- function(x) x
    plot_model <- function(x) x
    # tar_source()                                               
    tar_option_set(packages = c("readr", "dplyr", "ggplot2"))  
    list(                                                      
      tar_target(file, "data.csv", format = "file"),           
      tar_target(data, get_data(file)),                        
      tar_target(model, fit_model(data)),                      
      tar_target(plot, plot_model(model, data))                
    )               
  })
  targets::tar_visnetwork()
})
```

## `targets`

Run pipeline with `tar_make()`

```{r}
#| echo: true
#| eval: false
targets::tar_make()
```

```{r}
targets::tar_dir({
  targets::tar_script({
    library(targets)
    write.csv(iris, "data.csv")
    get_data <- function(file) {
      read.csv(file)
    }
    fit_model <- function(data) {
      Sys.sleep(3)
      lm(Petal.Width ~ Species, data = data)
    } 
    plot_model <- function(model, data) {
      plot(model)
      data
    } 
    # tar_source()
    tar_option_set(packages = c("readr", "dplyr", "ggplot2"))
    list(
      tar_target(file, "data.csv", format = "file"),
      tar_target(data, get_data(file)),
      tar_target(model, fit_model(data)),
      tar_target(plot, plot_model(model, data))
    )
  })
  targets::tar_make(reporter = "silent")
  targets::tar_invalidate(c(model, plot))
  targets::tar_make()
})
```

::: notes
In this example, I've made changes to `fit_model()` and re-run the pipeline.
Only that target and targets downstream have to be re-run.
:::

## Parallel execution with `crew`

We can set up a `crew` controller to run targets in parallel.

``` {.r filename="_targets.R"}
library(targets)
tar_source()                                              
tar_option_set(
  packages = c("readr", "dplyr", "ggplot2"),
  controller = crew::crew_controller_local(workers = 3) # <1>
) 
list(                                                     
  tar_target(file, "data.csv", format = "file"),          
  tar_target(data, get_data(file)),                       
  tar_target(model1, fit_model1(data)),   #<2>                  
  tar_target(model2, fit_model2(data)), #<2>
  tar_target(model3, fit_model3(data)) #<2>
)  
```

1.  This will set up three R sessions that can run tasks in parallel
2.  These three targets can all be run in parallel

::: {.callout-tip appearance="simple"}
This "local" controller also works on Open OnDemand!
:::

## `crew` technical details

```{mermaid}
%%| classes: vis-output
flowchart LR
    C("<img src='https://wlandau.github.io/crew/logo.svg' width='173' height='200' />" Distributed worker launchers for R) --> |powered by| D
    D("<img src='https://shikokuchuo.net/mirai/logo.png' width='173' height='200' />" Minimalist async evaluation for R) --> |powered by| E
    E("<img src='https://shikokuchuo.net/nanonext/logo.png' width='173' height='200' /> R binding for NNG (Nanomsg Next Gen)")
    click C href "https://wlandau.github.io/crew/"
    click D href "https://shikokuchuo.net/mirai/"
    click E href "https://shikokuchuo.net/nanonext/"
```

::: notes
-   The `mirai` package is a "minimalist async evaluation framework for R" powered by `NNG` (Nanomsg Next Gen)

-   `crew` extends `mirai` with a unifying interface for worker launchers

-   `crew` is extended to work with cluster computing (`crew.cluster`) and AWS (`crew.aws.batch`)
:::

## On the HPC with `crew.cluster`

Use SLURM (or PBS, SGE, etc.) without writing a bash script!

``` r
crew.cluster::crew_controller_slurm(
  workers = 5,
  slurm_partition = "standard",
  slurm_time_minutes = 1200,
  slurm_log_output = "logs/crew_log_%A.out",
  slurm_log_error = "logs/crew_log_%A.err",
  slurm_memory_gigabytes_per_cpu = 5, 
  slurm_cpus_per_task = 2,
  script_lines = c(
    "#SBATCH --account kristinariemer", 
    "module load R"
  )
)
```

. . . <!--# possibly put these on a separate slide with more info about optimization for HPC.  E.g. setting storage and retrival to "worker" in tar_option_set() -->

-   Persistent or semi-transient workers
-   Different controllers for different sized tasks

## Template repository

{{< iconify simple-icons:github >}} [cct-datascience/targets-uahpc](https://github.com/cct-datascience/targets-uahpc)

-   Links to relevant tutorials for prerequisite skills

-   Example `targets` pipeline

-   Uses `renv` for package management

-   Example `crew` controllers with all required fields set

-   Includes `run.sh` to launch `targets::tar_make()` as a SLURM job

## How we can help bridge the gap

-   Collaborative workshops led by HPC professionals & Data Scientists

-   Offer workshops on using HPC *without* the command line

-   Target HPC workshops to R/RStudio users

-   Create a template repo for using `targets` on *your* HPC

[^1]: Or Matlab, VSCode, etc.
